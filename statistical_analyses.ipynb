{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import ConcatDataset\n",
    "from PIL import Image\n",
    "import os\n",
    "import torchvision.models as models\n",
    "import time\n",
    "import copy\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "from dataloading import load_data\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path:  ../data/vegetable_images/train\n",
      "needed_length: 7500, expected_length_per_class: 500\n",
      "length of final dataset: 7500\n",
      "path:  ../data/vegetable_images/validation\n",
      "needed_length: 1500, expected_length_per_class: 100\n",
      "length of final dataset: 1500\n",
      "path:  ../data/vegetable_images/test\n",
      "needed_length: 1500, expected_length_per_class: 100\n",
      "length of final dataset: 1500\n"
     ]
    }
   ],
   "source": [
    "\n",
    "transform = transforms.Compose([ \n",
    "    transforms.Resize((227, 227)),  \n",
    "    transforms.ToTensor()\n",
    "])    \n",
    "\n",
    "data_percentage = 50\n",
    "\n",
    "\n",
    "train_loader, _, _ = load_data(data_dir = '../data/vegetable_images',\n",
    "                           batch_size = 64,\n",
    "                           data_type = \"train\",\n",
    "                           noise_type = \"None\",\n",
    "                           noise_percentage = 0,                           \n",
    "                           transform = transform,                           \n",
    "                           data_percentage=data_percentage)\n",
    "\n",
    "valid_loader, _, _ = load_data(data_dir = '../data/vegetable_images',\n",
    "                           batch_size = 64,\n",
    "                           data_type = \"validation\",\n",
    "                           noise_type = \"None\",\n",
    "                           noise_percentage = 0,                           \n",
    "                           transform = transform,                           \n",
    "                           data_percentage=data_percentage)\n",
    "\n",
    "test_loader, _, _ = load_data(data_dir = '../data/vegetable_images',\n",
    "                           batch_size = 64,\n",
    "                           data_type = \"test\",\n",
    "                           noise_type = \"None\",\n",
    "                           noise_percentage = 0,                           \n",
    "                           transform = transform,                           \n",
    "                           data_percentage=data_percentage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "concat_dataset = ConcatDataset([train_loader.dataset, valid_loader.dataset, test_loader.dataset])\n",
    "\n",
    "# Create a new data loader from the concatenated dataset\n",
    "batch_size = 64  # Set your desired batch size\n",
    "concat_loader = DataLoader(concat_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7552, 3, 51529])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total = []\n",
    "for images, _ in train_loader:\n",
    "    images, _ = next(iter(concat_loader))\n",
    "    flattened_image = images.view(images.size(0), images.size(1), -1)\n",
    "    total.append(flattened_image)\n",
    "pixel_values = torch.cat(total, dim=0)\n",
    "pixel_values.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 51529])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean = torch.mean(pixel_values, dim=0)\n",
    "mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4691, 0.4631, 0.3419])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_per_channel = mean.mean(dim=1)\n",
    "mean_per_channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "std = torch.std(pixel_values, dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0539, 0.0516, 0.0573])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std_sq = torch.square(std)\n",
    "std_mean = std_sq.mean(dim = 1)\n",
    "std_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2322, 0.2272, 0.2394])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std_mean = torch.sqrt(std_mean)\n",
    "std_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
