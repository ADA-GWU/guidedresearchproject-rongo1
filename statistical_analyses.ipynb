{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import ConcatDataset\n",
    "from PIL import Image\n",
    "import os\n",
    "import torchvision.models as models\n",
    "import time\n",
    "import copy\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "from dataloading import load_data\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path:  ../data/vegetable_images/train\n",
      "needed_length: 7500, expected_length_per_class: 500\n",
      "length of final dataset: 7500\n",
      "path:  ../data/vegetable_images/validation\n",
      "needed_length: 1500, expected_length_per_class: 100\n",
      "length of final dataset: 1500\n",
      "path:  ../data/vegetable_images/test\n",
      "needed_length: 1500, expected_length_per_class: 100\n",
      "length of final dataset: 1500\n"
     ]
    }
   ],
   "source": [
    "\n",
    "transform = transforms.Compose([ \n",
    "    transforms.Resize((227, 227)),  \n",
    "    transforms.ToTensor()\n",
    "])    \n",
    "\n",
    "data_percentage = 50\n",
    "\n",
    "\n",
    "train_loader, _, _ = load_data(data_dir = '../data/vegetable_images',\n",
    "                           batch_size = 64,\n",
    "                           data_type = \"train\",\n",
    "                           noise_type = \"None\",\n",
    "                           noise_percentage = 0,                           \n",
    "                           transform = transform,                           \n",
    "                           data_percentage=data_percentage)\n",
    "\n",
    "valid_loader, _, _ = load_data(data_dir = '../data/vegetable_images',\n",
    "                           batch_size = 64,\n",
    "                           data_type = \"validation\",\n",
    "                           noise_type = \"None\",\n",
    "                           noise_percentage = 0,                           \n",
    "                           transform = transform,                           \n",
    "                           data_percentage=data_percentage)\n",
    "\n",
    "test_loader, _, _ = load_data(data_dir = '../data/vegetable_images',\n",
    "                           batch_size = 64,\n",
    "                           data_type = \"test\",\n",
    "                           noise_type = \"None\",\n",
    "                           noise_percentage = 0,                           \n",
    "                           transform = transform,                           \n",
    "                           data_percentage=data_percentage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "concat_dataset = ConcatDataset([train_loader.dataset, valid_loader.dataset, test_loader.dataset])\n",
    "\n",
    "# Create a new data loader from the concatenated dataset\n",
    "batch_size = 64  # Set your desired batch size\n",
    "concat_loader = DataLoader(concat_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 3, 227, 227])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images, _ = next(iter(concat_loader))\n",
    "images.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 51529])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.3294, 0.3216, 0.3137,  ..., 0.1686, 0.2000, 0.1922],\n",
       "         [0.3569, 0.3569, 0.3569,  ..., 0.3490, 0.3882, 0.3804],\n",
       "         [0.4667, 0.4706, 0.4824,  ..., 0.2039, 0.2314, 0.2235]],\n",
       "\n",
       "        [[0.0078, 0.0000, 0.0000,  ..., 0.0706, 0.0706, 0.0667],\n",
       "         [0.0078, 0.0000, 0.0000,  ..., 0.1294, 0.1333, 0.1294],\n",
       "         [0.0078, 0.0000, 0.0000,  ..., 0.0157, 0.0196, 0.0157]],\n",
       "\n",
       "        [[0.2510, 0.4667, 0.4824,  ..., 0.7098, 0.7059, 0.7059],\n",
       "         [0.2745, 0.4863, 0.4941,  ..., 0.7373, 0.7333, 0.7333],\n",
       "         [0.1412, 0.3608, 0.3804,  ..., 0.7765, 0.7725, 0.7725]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.6275, 0.6314, 0.6392,  ..., 0.5922, 0.5961, 0.5961],\n",
       "         [0.6235, 0.6275, 0.6353,  ..., 0.6000, 0.6000, 0.6000],\n",
       "         [0.6431, 0.6471, 0.6549,  ..., 0.6588, 0.6706, 0.6706]],\n",
       "\n",
       "        [[0.1373, 0.1569, 0.1804,  ..., 0.0039, 0.0431, 0.0314],\n",
       "         [0.1451, 0.1647, 0.1961,  ..., 0.0000, 0.0235, 0.0039],\n",
       "         [0.1961, 0.2157, 0.2431,  ..., 0.0000, 0.0078, 0.0000]],\n",
       "\n",
       "        [[0.4863, 0.5451, 0.5804,  ..., 0.7255, 0.4510, 0.1137],\n",
       "         [0.5255, 0.5843, 0.6196,  ..., 0.7098, 0.4157, 0.0745],\n",
       "         [0.5647, 0.6235, 0.6588,  ..., 0.6745, 0.3804, 0.0353]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flattened_tensor1 = images.view(images.size(0), images.size(1), -1)\n",
    "\n",
    "print(flattened_tensor1.shape)\n",
    "flattened_tensor1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 51529])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.2392, 0.2510, 0.2941,  ..., 0.6314, 0.6353, 0.6353],\n",
       "         [0.2549, 0.2784, 0.3294,  ..., 0.7333, 0.7373, 0.7373],\n",
       "         [0.0549, 0.1059, 0.2118,  ..., 0.8314, 0.8353, 0.8353]],\n",
       "\n",
       "        [[0.4431, 0.5176, 0.5765,  ..., 0.0157, 0.0627, 0.0863],\n",
       "         [0.4196, 0.4863, 0.5373,  ..., 0.0627, 0.0980, 0.1216],\n",
       "         [0.4353, 0.4784, 0.4980,  ..., 0.0000, 0.0235, 0.0471]],\n",
       "\n",
       "        [[0.0314, 0.0510, 0.0353,  ..., 0.7137, 0.7137, 0.7098],\n",
       "         [0.0549, 0.0745, 0.0706,  ..., 0.7569, 0.7569, 0.7529],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.3882, 0.3961, 0.3922]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.3647, 0.3765, 0.4314,  ..., 0.1098, 0.1451, 0.2863],\n",
       "         [0.3333, 0.3451, 0.4039,  ..., 0.0000, 0.0431, 0.1922],\n",
       "         [0.3765, 0.3882, 0.4353,  ..., 0.0118, 0.0471, 0.1922]],\n",
       "\n",
       "        [[0.3765, 0.2314, 0.1137,  ..., 0.1059, 0.1098, 0.0941],\n",
       "         [0.4863, 0.3255, 0.1843,  ..., 0.2157, 0.2196, 0.2039],\n",
       "         [0.1961, 0.0824, 0.0314,  ..., 0.0549, 0.0706, 0.0549]],\n",
       "\n",
       "        [[0.6196, 0.5961, 0.5333,  ..., 0.1529, 0.2118, 0.2275],\n",
       "         [0.7843, 0.7725, 0.7216,  ..., 0.2863, 0.3529, 0.3686],\n",
       "         [0.4000, 0.3686, 0.2745,  ..., 0.0588, 0.1137, 0.1294]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images, _ = next(iter(concat_loader))\n",
    "flattened_tensor2 = images.view(images.size(0), images.size(1), -1)\n",
    "\n",
    "print(flattened_tensor2.shape)\n",
    "flattened_tensor2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[0.3294, 0.3216, 0.3137,  ..., 0.1686, 0.2000, 0.1922],\n",
       "          [0.3569, 0.3569, 0.3569,  ..., 0.3490, 0.3882, 0.3804],\n",
       "          [0.4667, 0.4706, 0.4824,  ..., 0.2039, 0.2314, 0.2235]],\n",
       " \n",
       "         [[0.0078, 0.0000, 0.0000,  ..., 0.0706, 0.0706, 0.0667],\n",
       "          [0.0078, 0.0000, 0.0000,  ..., 0.1294, 0.1333, 0.1294],\n",
       "          [0.0078, 0.0000, 0.0000,  ..., 0.0157, 0.0196, 0.0157]],\n",
       " \n",
       "         [[0.2510, 0.4667, 0.4824,  ..., 0.7098, 0.7059, 0.7059],\n",
       "          [0.2745, 0.4863, 0.4941,  ..., 0.7373, 0.7333, 0.7333],\n",
       "          [0.1412, 0.3608, 0.3804,  ..., 0.7765, 0.7725, 0.7725]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.6275, 0.6314, 0.6392,  ..., 0.5922, 0.5961, 0.5961],\n",
       "          [0.6235, 0.6275, 0.6353,  ..., 0.6000, 0.6000, 0.6000],\n",
       "          [0.6431, 0.6471, 0.6549,  ..., 0.6588, 0.6706, 0.6706]],\n",
       " \n",
       "         [[0.1373, 0.1569, 0.1804,  ..., 0.0039, 0.0431, 0.0314],\n",
       "          [0.1451, 0.1647, 0.1961,  ..., 0.0000, 0.0235, 0.0039],\n",
       "          [0.1961, 0.2157, 0.2431,  ..., 0.0000, 0.0078, 0.0000]],\n",
       " \n",
       "         [[0.4863, 0.5451, 0.5804,  ..., 0.7255, 0.4510, 0.1137],\n",
       "          [0.5255, 0.5843, 0.6196,  ..., 0.7098, 0.4157, 0.0745],\n",
       "          [0.5647, 0.6235, 0.6588,  ..., 0.6745, 0.3804, 0.0353]]]),\n",
       " tensor([[[0.2392, 0.2510, 0.2941,  ..., 0.6314, 0.6353, 0.6353],\n",
       "          [0.2549, 0.2784, 0.3294,  ..., 0.7333, 0.7373, 0.7373],\n",
       "          [0.0549, 0.1059, 0.2118,  ..., 0.8314, 0.8353, 0.8353]],\n",
       " \n",
       "         [[0.4431, 0.5176, 0.5765,  ..., 0.0157, 0.0627, 0.0863],\n",
       "          [0.4196, 0.4863, 0.5373,  ..., 0.0627, 0.0980, 0.1216],\n",
       "          [0.4353, 0.4784, 0.4980,  ..., 0.0000, 0.0235, 0.0471]],\n",
       " \n",
       "         [[0.0314, 0.0510, 0.0353,  ..., 0.7137, 0.7137, 0.7098],\n",
       "          [0.0549, 0.0745, 0.0706,  ..., 0.7569, 0.7569, 0.7529],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.3882, 0.3961, 0.3922]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.3647, 0.3765, 0.4314,  ..., 0.1098, 0.1451, 0.2863],\n",
       "          [0.3333, 0.3451, 0.4039,  ..., 0.0000, 0.0431, 0.1922],\n",
       "          [0.3765, 0.3882, 0.4353,  ..., 0.0118, 0.0471, 0.1922]],\n",
       " \n",
       "         [[0.3765, 0.2314, 0.1137,  ..., 0.1059, 0.1098, 0.0941],\n",
       "          [0.4863, 0.3255, 0.1843,  ..., 0.2157, 0.2196, 0.2039],\n",
       "          [0.1961, 0.0824, 0.0314,  ..., 0.0549, 0.0706, 0.0549]],\n",
       " \n",
       "         [[0.6196, 0.5961, 0.5333,  ..., 0.1529, 0.2118, 0.2275],\n",
       "          [0.7843, 0.7725, 0.7216,  ..., 0.2863, 0.3529, 0.3686],\n",
       "          [0.4000, 0.3686, 0.2745,  ..., 0.0588, 0.1137, 0.1294]]])]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pixel_values = []\n",
    "pixel_values.append(flattened_tensor1)\n",
    "pixel_values.append(flattened_tensor2)\n",
    "pixel_values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 3, 51529])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pixel_values = torch.cat(pixel_values, dim=0)\n",
    "pixel_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 51529])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean = torch.mean(pixel_values, dim=0)\n",
    "mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4649, 0.4587, 0.3365])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_per_channel = mean.mean(dim=1)\n",
    "mean_per_channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7552, 3, 51529])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total = []\n",
    "for images, _ in train_loader:\n",
    "    images, _ = next(iter(concat_loader))\n",
    "    flattened_image = images.view(images.size(0), images.size(1), -1)\n",
    "    total.append(flattened_image)\n",
    "pixel_values = torch.cat(total, dim=0)\n",
    "pixel_values.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 51529])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean = torch.mean(pixel_values, dim=0)\n",
    "mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4680, 0.4647, 0.3441])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_per_channel = mean.mean(dim=1)\n",
    "mean_per_channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2315, 0.2262, 0.2399])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std = torch.std(pixel_values, dim=0)\n",
    "std_per_channel = std.mean(dim=1)\n",
    "std_per_channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at ..\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 4669764096 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Lenovo\\Desktop\\ada\\research\\guidedresearchproject-yusif_mukhtarov\\statistical_analyses.ipynb Cell 15\u001b[0m in \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Lenovo/Desktop/ada/research/guidedresearchproject-yusif_mukhtarov/statistical_analyses.ipynb#X41sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m reshaped_pixel \u001b[39m=\u001b[39m pixel_values\u001b[39m.\u001b[39mview(\u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Lenovo/Desktop/ada/research/guidedresearchproject-yusif_mukhtarov/statistical_analyses.ipynb#X41sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m std \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mstd(reshaped_pixel, dim\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Lenovo/Desktop/ada/research/guidedresearchproject-yusif_mukhtarov/statistical_analyses.ipynb#X41sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m std\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at ..\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 4669764096 bytes."
     ]
    }
   ],
   "source": [
    "reshaped_pixel = pixel_values.view(1, 3, -1)\n",
    "std = torch.std(reshaped_pixel, dim=0)\n",
    "std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at ..\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 4669764096 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Lenovo\\Desktop\\ada\\research\\guidedresearchproject-yusif_mukhtarov\\statistical_analyses.ipynb Cell 16\u001b[0m in \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Lenovo/Desktop/ada/research/guidedresearchproject-yusif_mukhtarov/statistical_analyses.ipynb#X42sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m mean \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mmean(reshaped_pixel, dim\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at ..\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 4669764096 bytes."
     ]
    }
   ],
   "source": [
    "mean = torch.mean(reshaped_pixel, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
